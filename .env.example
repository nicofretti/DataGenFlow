# llm configuration (openai-compatible format)
# for ollama: http://localhost:11434/v1/chat/completions
# for openai: https://api.openai.com/v1/chat/completions
# for anthropic: https://api.anthropic.com/v1/messages
LLM_ENDPOINT=http://localhost:11434/v1/chat/completions
LLM_API_KEY=
LLM_MODEL=llama3

# database
DATABASE_PATH=data/qa_records.db

# server
HOST=0.0.0.0
PORT=8000

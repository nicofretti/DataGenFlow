# llm configuration
# for testing with mock server: http://localhost:11434/api/generate
# for ollama: http://localhost:11434/api/generate
# for openai: https://api.openai.com/v1/chat/completions
LLM_ENDPOINT=http://localhost:11434/api/generate
LLM_API_KEY=
LLM_MODEL=llama3

# database
DATABASE_PATH=data/qa_records.db

# server
HOST=0.0.0.0
PORT=8000
